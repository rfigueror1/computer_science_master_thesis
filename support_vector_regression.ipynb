{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "count_vect = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = import_data.import_data()\n",
    "first_set_array = all_data[0]\n",
    "second_set_array = all_data[1]\n",
    "third_set_array = all_data[2]\n",
    "fourth_set_array = all_data[3]\n",
    "fifth_set_array = all_data[4]\n",
    "sixth_set_array = all_data[5]\n",
    "seventh_set_array = all_data[6]\n",
    "eight_set_array = all_data[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = [i for i in first_set_array[1:,2]] \\\n",
    "+ [i for i in second_set_array[1:,2]] + [i for i in third_set_array[1:,2]] \\\n",
    "+ [i for i in fourth_set_array[1:,2]] + [i for i in fifth_set_array[1:,2]] \\\n",
    "+ [i for i in sixth_set_array[1:,2]] + [i for i in seventh_set_array[1:,2]] \\\n",
    "+ [i for i in eight_set_array[1:,2]]\n",
    "\n",
    "all_set_index = [i for i in first_set_array[1:,1]] + [i for i in second_set_array[1:,1]] \\\n",
    "+ [i for i in second_set_array[1:,1]] + [i for i in third_set_array[1:,1]] \\\n",
    "+ [i for i in fourth_set_array[1:,1]] + [i for i in fifth_set_array[1:,1]] \\\n",
    "+ [i for i in sixth_set_array[1:,1]] + [i for i in seventh_set_array[1:,1]] \\\n",
    "+ [i for i in eight_set_array[1:,1]]\n",
    "\n",
    "all_first_domain = [i for i in first_set_array[1:,6]] + [i for i in second_set_array[1:,6]] \\\n",
    "    + [i for i in third_set_array[1:,6]] + [i for i in fourth_set_array[1:,6]] \\\n",
    "    + [i for i in fifth_set_array[1:,6]] + [i for i in sixth_set_array[1:,6]] \\\n",
    "    + [i for i in seventh_set_array[1:,6]] + [i for i in eight_set_array[1:,6]]\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "all_first_domain = pd.DataFrame(all_first_domain)\n",
    "all_first_domain_stnd = scaler.fit_transform(all_first_domain)\n",
    "\n",
    "# shuffle the data\n",
    "# Given list1 and list2\n",
    "all_text_shuf = []\n",
    "all_first_domain_shuf = []\n",
    "all_set_index_shuf = []\n",
    "index_shuf = list(range(len(all_first_domain)))\n",
    "shuffle(index_shuf)\n",
    "for i in index_shuf:\n",
    "  all_text_shuf.append(all_text[i])\n",
    "  all_set_index_shuf.append(all_set_index[i])\n",
    "  all_first_domain_shuf.append(all_first_domain_stnd[i])\n",
    "\n",
    "seventy_percent = round(len(all_first_domain_shuf)*0.8)\n",
    "thirty_percent = round(len(all_first_domain_shuf)*0.2)\n",
    "\n",
    "# all_text_train = all_text[0:9000]\n",
    "all_text_train = all_text_shuf[0:seventy_percent-1]\n",
    "\n",
    "# all_text_train_labels = all_first_domain_stnd[0:9000]\n",
    "all_text_train_labels = all_first_domain_shuf[0:seventy_percent-1]\n",
    "\n",
    "all_text_train_sets = all_set_index_shuf[0:seventy_percent-1]\n",
    "\n",
    "# all_text_test_labels = all_first_domain_stnd[9001:]\n",
    "all_text_test_labels = all_first_domain_shuf[seventy_percent:]\n",
    "\n",
    "# all_text_test = all_text[9001:]\n",
    "all_text_test = all_text_shuf[seventy_percent:]\n",
    "\n",
    "all_text_sets_sets = all_set_index_shuf[seventy_percent:]\n",
    "\n",
    "all_text_train = [str(elem) for elem in all_text_train]\n",
    "data_train1 = [all_text_train, all_text_train_labels, all_text_train_sets]\n",
    "data_train1 =[list(i) for i in zip(*data_train1)]\n",
    "len(all_text_train)\n",
    "len(all_text_test)\n",
    "\n",
    "# Now we are going to fit BERT into this new sets with 4 labels\n",
    "\n",
    "all_text_test = [str(elem) for elem in all_text_test]\n",
    "data_test1 = [all_text_test, all_text_test_labels, all_text_sets_sets]\n",
    "data_test1 =[list(i) for i in zip(*data_test1)]\n",
    "\n",
    ">>> text_clf_svr = Pipeline([('vect', CountVectorizer()),\n",
    "...                      ('tfidf', TfidfTransformer()),\n",
    "...                      ('clf-svm', SVR(kernel='linear', C=100, gamma='auto')),\n",
    "... ])\n",
    "\n",
    ">>> _ = text_clf_svr.fit(all_text_train, all_text_train_labels)\n",
    "\n",
    "predicted_my_data = text_clf_svr.predict(all_text_test)\n",
    "mean_squared_error(all_text_test_labels, predicted_my_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
